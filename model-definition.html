<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Algorithms Reference</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #f5f7fa 0%, #e3f2fd 100%);
            padding: 20px;
            min-height: 100vh;

            /* same center look & width feel as previous layout */
            display: flex;
            justify-content: center;
        }

        .container {
            max-width: 1100px;  /* MATCHED with previous cheat sheet width */
            width: 100%;
            background: #ffffff;
            padding: 25px 30px;  /* matched spacing */
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }

        h1 {
            text-align: center;
            color: #1f2937;
            font-size: 2rem; /* matched heading scale */
            margin-bottom: 8px;
        }

        .subtitle {
            text-align: center;
            color: #6b7280;
            margin-bottom: 20px;
            font-size: 1rem;
        }

        .table-wrapper {
            background: white;
            border-radius: 12px;
            box-shadow: 0 8px 18px rgba(0, 0, 0, 0.08);
            overflow: hidden;
        }

        .table-container {
            overflow-x: auto;
        }

        table {
            width: 100%;
            border-collapse: collapse;
        }

        thead {
            background: linear-gradient(135deg, #2563eb 0%, #4f46e5 100%);
        }

        thead th {
            color: white;
            padding: 18px;       /* reduced to match previous card density */
            text-align: left;
            font-size: 1rem;
            font-weight: 600;
        }

        tbody tr {
            border-bottom: 1px solid #e5e7eb;
            transition: background-color 0.3s ease;
        }

        tbody tr:nth-child(even) {
            background-color: #f9fafb;
        }

        tbody tr:hover {
            background-color: #dbeafe;
        }

        tbody td {
            padding: 18px;      /* matched proportions */
            color: #374151;
            font-size: 0.9rem;
        }

        tbody td:first-child {
            font-weight: 600;
            color: #2563eb;
            width: 20%;
        }

        tbody td:nth-child(2) {
            width: 50%;
            line-height: 1.55;
        }

        tbody td:nth-child(3) {
            width: 30%;
            font-family: 'Courier New', monospace;
            font-size: 0.85rem;
            background-color: #f3f4f6;
            border-radius: 6px;
            padding: 14px;
        }

        .footer {
            text-align: center;
            margin-top: 25px;
            color: #6b7280;
            font-size: 0.85rem;
        }

        .footer p {
            margin: 5px 0;
        }

        @media (max-width: 768px) {
            .container {
                padding: 18px;
            }

            h1 {
                font-size: 1.6rem;
            }

            thead th,
            tbody td {
                padding: 12px;
                font-size: 0.8rem;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Machine Learning Algorithms Reference</h1>
        <p class="subtitle">Comprehensive guide to ML models with definitions and formulas</p>

        <div class="table-wrapper">
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Algorithm Name</th>
                            <th>Definition</th>
                            <th>Key Formula / Logic</th>
                        </tr>
                    </thead>
                    <tbody>

                        <!-- ALL YOUR ROWS ARE KEPT SAME -->
                        <tr>
                            <td>Linear Regression</td>
                            <td>A supervised learning algorithm that models the relationship between a dependent variable and one or more independent variables using a linear equation.</td>
                            <td>Y = β₀ + β₁X₁ + ... + βₙXₙ</td>
                        </tr>

                        <tr>
                            <td>Logistic Regression</td>
                            <td>Supervised classification algorithm predicting binary outcomes using the sigmoid function.</td>
                            <td>P = 1 / (1 + e^-(β₀ + β₁X₁ + ...))</td>
                        </tr>

                        <tr>
                            <td>Decision Tree</td>
                            <td>A supervised model that splits data recursively based on feature values.</td>
                            <td>Binary recursive splits using thresholds</td>
                        </tr>

                        <tr>
                            <td>Random Forest</td>
                            <td>Ensemble of decision trees using bagging to reduce variance.</td>
                            <td>Ŷ = (1/T) Σ predictions</td>
                        </tr>

                        <tr>
                            <td>Gradient Boosting</td>
                            <td>Sequential ensemble where each model fixes errors of the previous ones.</td>
                            <td>F(x) = Σ αₘ hₘ(x)</td>
                        </tr>

                        <tr>
                            <td>SVM</td>
                            <td>Finds the maximum-margin hyperplane separating classes.</td>
                            <td>yᵢ(w·xᵢ + b) ≥ 1</td>
                        </tr>

                        <tr>
                            <td>KNN</td>
                            <td>Classifies using the majority vote of the K closest neighbors.</td>
                            <td>d = √Σ(xᵢ − yᵢ)²</td>
                        </tr>

                        <tr>
                            <td>Naive Bayes</td>
                            <td>Probabilistic classifier assuming feature independence.</td>
                            <td>P(A|B) = P(B|A)P(A)/P(B)</td>
                        </tr>

                        <tr>
                            <td>K-Means</td>
                            <td>Unsupervised clustering minimizing within-cluster variance.</td>
                            <td>Σ Σ ||xᵢ - μₖ||²</td>
                        </tr>

                        <tr>
                            <td>Hierarchical Clustering</td>
                            <td>Builds clusters using bottom-up or top-down strategies.</td>
                            <td>d(A,B) by linkage (single/complete/average)</td>
                        </tr>

                        <tr>
                            <td>PCA</td>
                            <td>Transforms data into principal components maximizing variance.</td>
                            <td>Eigenvectors of covariance matrix</td>
                        </tr>

                        <tr>
                            <td>Neural Networks</td>
                            <td>Feedforward networks that learn weights via backpropagation.</td>
                            <td>activation(Σ(wᵢxᵢ + b))</td>
                        </tr>

                        <tr>
                            <td>CNN</td>
                            <td>Deep model with filters for spatial pattern detection.</td>
                            <td>Convolution: (f * g)(x) = Σ f(a)g(x-a)</td>
                        </tr>

                        <tr>
                            <td>RNN</td>
                            <td>Sequential model capturing time dependencies.</td>
                            <td>hₜ = tanh(Wₓₕxₜ + Wₕₕhₜ₋₁)</td>
                        </tr>

                        <tr>
                            <td>Transformer</td>
                            <td>Deep model using self-attention, basis of modern NLP.</td>
                            <td>softmax(QKᵀ/√dₖ)V</td>
                        </tr>

                        <tr>
                            <td>Autoencoders</td>
                            <td>Unsupervised network learning compressed representations.</td>
                            <td>Minimize ||X - reconstruction||²</td>
                        </tr>

                        <tr>
                            <td>DBSCAN</td>
                            <td>Groups dense regions and labels sparse ones as noise.</td>
                            <td>ε-neighborhood ≥ minPts</td>
                        </tr>

                    </tbody>
                </table>
            </div>
        </div>

        <div class="footer">
            <p><strong>Total Algorithms: 17</strong></p>
            <p>Includes Supervised, Unsupervised, and Deep Learning Methods</p>
        </div>
    </div>
</body>
</html>
